{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyQtljP-qPHY",
    "outputId": "8a87f212-7215-410d-dcd3-d12a2fb5eed3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Use the latest version of pip.\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade \"tfx[kfp]\"\n",
    "# !pip install --upgrade tensorflow\n",
    "# !pip install --upgrade tensorflow_transform\n",
    "\n",
    "# !pip install --upgrade pyarrow\n",
    "# !pip install --upgrade apache-beam\n",
    "# !pip install --upgrade tensorflow_model_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install apache-beam pyarrow tensorflow_model_analysis tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyQtljP-qPHY",
    "outputId": "8a87f212-7215-410d-dcd3-d12a2fb5eed3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-model-analysis==0.46.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyQtljP-qPHY",
    "outputId": "8a87f212-7215-410d-dcd3-d12a2fb5eed3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyQtljP-qPHY",
    "outputId": "8a87f212-7215-410d-dcd3-d12a2fb5eed3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r final_r.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow_model_analysis as tfma\n",
    "# from tfx import v1 as tfx\n",
    "\n",
    "# !pip uninstall pandas -y\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyQtljP-qPHY",
    "outputId": "8a87f212-7215-410d-dcd3-d12a2fb5eed3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 18:03:03.954500: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-21 18:03:04.840125: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-21 18:03:04.840292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-21 18:03:05.026063: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-21 18:03:05.408593: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-21 18:03:05.412788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx import v1 as tfx\n",
    "from tfx.types import Channel, standard_artifacts\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.components import Evaluator\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.proto import trainer_pb2, pusher_pb2\n",
    "from tfx.types import standard_artifacts\n",
    "# import tensorflow_model_analysis as tfma\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kZQA0KrfXCvU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_SveIKxaENu"
   },
   "source": [
    "### Check the package versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xd-iP9wEaENu",
    "outputId": "4582eaeb-e6a1-46a5-ca31-ada3a64359c7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.1\n",
      "TFX version: 1.15.1\n",
      "KFP version: 1.8.22\n",
      "TFMA version: 0.46.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import kfp\n",
    "print('KFP version: {}'.format(kfp.__version__))\n",
    "print('TFMA version: {}'.format(tfma.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EcUseqJaE2XN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'jovial-sight-429218-i7'\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'\n",
    "GCS_BUCKET_NAME = 'mlops_final_run'\n",
    "FILE_NAME ='Final_Chicago_Train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAaCPLjgiJrO"
   },
   "source": [
    "#### Set `gcloud` to use your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkWdxe4TXRHk",
    "outputId": "bd93c29f-9870-45da-eab2-7180b8b01eca",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jovial-sight-429218-i7'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " {GOOGLE_CLOUD_PROJECT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_x_8xoj6k0L"
   },
   "source": [
    "### Set up Global variables for model serving locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPN6UL5CazNy",
    "outputId": "75956013-1a23-4e03-9162-99b0709e2dcf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS directories created:\n",
      "gs://mlops_final_run/chicago-gcp-vertex-pipelines/pipeline_root/\n",
      "gs://mlops_final_run/chicago-gcp-vertex-pipelines/pipeline_module/\n",
      "gs://mlops_final_run/chicago-gcp-vertex-pipelines/data/\n",
      "gs://mlops_final_run/chicago-gcp-vertex-pipelines/serving_model/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "\n",
    "PIPELINE_NAME = 'chicago-gcp-vertex-pipelines'\n",
    "\n",
    "# Function to create directories in GCS\n",
    "def create_gcs_directory(bucket_name, path):\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string('', content_type='application/x-www-form-urlencoded;charset=UTF-8')\n",
    "\n",
    "# Paths for pipeline artifacts\n",
    "PIPELINE_ROOT = f'gs://{GCS_BUCKET_NAME}/{PIPELINE_NAME}/pipeline_root/'\n",
    "MODULE_ROOT = f'gs://{GCS_BUCKET_NAME}/{PIPELINE_NAME}/pipeline_module/'\n",
    "DATA_ROOT = f'gs://{GCS_BUCKET_NAME}/{PIPELINE_NAME}/data/'\n",
    "SERVING_MODEL_DIR = f'gs://{GCS_BUCKET_NAME}/{PIPELINE_NAME}/serving_model/'\n",
    "\n",
    "# Create directories\n",
    "create_gcs_directory(GCS_BUCKET_NAME, f'{PIPELINE_NAME}/pipeline_root/')\n",
    "create_gcs_directory(GCS_BUCKET_NAME, f'{PIPELINE_NAME}/pipeline_module/')\n",
    "create_gcs_directory(GCS_BUCKET_NAME, f'{PIPELINE_NAME}/data/')\n",
    "create_gcs_directory(GCS_BUCKET_NAME, f'{PIPELINE_NAME}/serving_model/')\n",
    "\n",
    "print('GCS directories created:')\n",
    "print(PIPELINE_ROOT)\n",
    "print(MODULE_ROOT)\n",
    "print(DATA_ROOT)\n",
    "print(SERVING_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11J7XiCq6AFP"
   },
   "source": [
    "We need to make our own copy of the dataset. Because TFX ExampleGen reads\n",
    "inputs from a directory, we need to create a directory and copy dataset to it\n",
    "on GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASpoNmxKSQjI"
   },
   "source": [
    "Take a quick look at the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>extras</th>\n",
       "      <th>trip_total</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a8f3ac8c8b7751248924252b6880ce393a3b486f</td>\n",
       "      <td>9f2cb5811cb0450b60400085b275296a3a307c2c26458f...</td>\n",
       "      <td>2019-05-09 13:00:00+00:00</td>\n",
       "      <td>2019-05-09 13:15:00+00:00</td>\n",
       "      <td>426</td>\n",
       "      <td>0.74</td>\n",
       "      <td>17031081402</td>\n",
       "      <td>17031081401</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Flash Cab</td>\n",
       "      <td>41.891972</td>\n",
       "      <td>-87.612945</td>\n",
       "      <td>POINT (-87.6129454143 41.8919715078)</td>\n",
       "      <td>41.895033</td>\n",
       "      <td>-87.619711</td>\n",
       "      <td>POINT (-87.6197106717 41.8950334495)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74e380607a4462d41cca62cd6beec24ed74357fd</td>\n",
       "      <td>501280246f63bb05a144c72919f7c442c3a50fd9cfc2c7...</td>\n",
       "      <td>2020-02-20 18:15:00+00:00</td>\n",
       "      <td>2020-02-20 18:30:00+00:00</td>\n",
       "      <td>585</td>\n",
       "      <td>1.57</td>\n",
       "      <td>17031081700</td>\n",
       "      <td>17031080201</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Flash Cab</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>POINT (-87.6318639497 41.8920421365)</td>\n",
       "      <td>41.909496</td>\n",
       "      <td>-87.630964</td>\n",
       "      <td>POINT (-87.630963601 41.9094956686)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69162dda4acf3f1a1616a709a6d92f4bfb689818</td>\n",
       "      <td>c26296b24f8bf5e5dd0751206051b0bac76a39d87ec36d...</td>\n",
       "      <td>2019-05-31 11:45:00+00:00</td>\n",
       "      <td>2019-05-31 11:45:00+00:00</td>\n",
       "      <td>315</td>\n",
       "      <td>0.61</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5c519eb8e532ffbc9f7c5c0bd0b2e1f5b76176f2</td>\n",
       "      <td>1833edab9ed696386a8224986104fd046eaf9ea8dffb9e...</td>\n",
       "      <td>2019-01-10 02:15:00+00:00</td>\n",
       "      <td>2019-01-10 02:15:00+00:00</td>\n",
       "      <td>420</td>\n",
       "      <td>0.10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.901207</td>\n",
       "      <td>-87.676356</td>\n",
       "      <td>POINT (-87.6763559892 41.90120699410001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7a8120b81ee6923fa0fc017fb61173bd307cea95</td>\n",
       "      <td>bf2d7795769ed240df920dce5605bda9e543e9cf05272a...</td>\n",
       "      <td>2021-12-17 16:15:00+00:00</td>\n",
       "      <td>2021-12-17 19:00:00+00:00</td>\n",
       "      <td>9720</td>\n",
       "      <td>104.30</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>269.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxicab Insurance Agency, LLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1b977c00fa841ed8c542b9a1b4ad088293f6c7c6</td>\n",
       "      <td>34a67a4a44a0c07c24568bbdee112b8e7877b6b49664a7...</td>\n",
       "      <td>2021-10-14 17:30:00+00:00</td>\n",
       "      <td>2021-10-14 18:30:00+00:00</td>\n",
       "      <td>3918</td>\n",
       "      <td>18.04</td>\n",
       "      <td>17031081500</td>\n",
       "      <td>17031980000</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Flash Cab</td>\n",
       "      <td>41.892508</td>\n",
       "      <td>-87.626215</td>\n",
       "      <td>POINT (-87.6262149064 41.8925077809)</td>\n",
       "      <td>41.979071</td>\n",
       "      <td>-87.903040</td>\n",
       "      <td>POINT (-87.9030396611 41.9790708201)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>b935a4a925089bf0ceb2036fdd87c17ddef89651</td>\n",
       "      <td>c0233d1acdf95f778faf4bcadcff7035dc6bbcab99f07e...</td>\n",
       "      <td>2023-07-13 12:45:00+00:00</td>\n",
       "      <td>2023-07-13 12:45:00+00:00</td>\n",
       "      <td>480</td>\n",
       "      <td>1.50</td>\n",
       "      <td>17031081201</td>\n",
       "      <td>17031081800</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>Cash</td>\n",
       "      <td>U Taxicab</td>\n",
       "      <td>41.899156</td>\n",
       "      <td>-87.626211</td>\n",
       "      <td>POINT (-87.6262105324 41.8991556134)</td>\n",
       "      <td>41.893216</td>\n",
       "      <td>-87.637844</td>\n",
       "      <td>POINT (-87.6378442095 41.8932163595)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>2619ebf9c87ce668bd78e9b816d5ac3e5831af66</td>\n",
       "      <td>e931cc8f41df1e511815b487341e4896fb01f01b2f8fc6...</td>\n",
       "      <td>2019-07-26 10:45:00+00:00</td>\n",
       "      <td>2019-07-26 10:45:00+00:00</td>\n",
       "      <td>480</td>\n",
       "      <td>1.20</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031081403</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Choice Taxi Association</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>41.890922</td>\n",
       "      <td>-87.618868</td>\n",
       "      <td>POINT (-87.6188683546 41.8909220259)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>14f5937595f891ff505aa906a418f3d61b8277c2</td>\n",
       "      <td>dd3ec76231a679ee3501f302592955447df6907cf2806e...</td>\n",
       "      <td>2019-07-18 13:00:00+00:00</td>\n",
       "      <td>2019-07-18 13:15:00+00:00</td>\n",
       "      <td>660</td>\n",
       "      <td>0.90</td>\n",
       "      <td>17031320100</td>\n",
       "      <td>17031081800</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Top Cab Affiliation</td>\n",
       "      <td>41.884987</td>\n",
       "      <td>-87.620993</td>\n",
       "      <td>POINT (-87.6209929134 41.8849871918)</td>\n",
       "      <td>41.893216</td>\n",
       "      <td>-87.637844</td>\n",
       "      <td>POINT (-87.6378442095 41.8932163595)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>45bbba42c344150aa18c4735277e8fa6b94e1018</td>\n",
       "      <td>e11ecc8e74a2d3cbd2e9f58151ed09c81587cb491c50f6...</td>\n",
       "      <td>2021-02-07 18:45:00+00:00</td>\n",
       "      <td>2021-02-07 18:45:00+00:00</td>\n",
       "      <td>480</td>\n",
       "      <td>1.70</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>42.009623</td>\n",
       "      <td>-87.670167</td>\n",
       "      <td>POINT (-87.6701668569 42.0096228806)</td>\n",
       "      <td>41.986712</td>\n",
       "      <td>-87.663416</td>\n",
       "      <td>POINT (-87.6634164054 41.9867117999)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     unique_key  \\\n",
       "0      a8f3ac8c8b7751248924252b6880ce393a3b486f   \n",
       "1      74e380607a4462d41cca62cd6beec24ed74357fd   \n",
       "2      69162dda4acf3f1a1616a709a6d92f4bfb689818   \n",
       "3      5c519eb8e532ffbc9f7c5c0bd0b2e1f5b76176f2   \n",
       "4      7a8120b81ee6923fa0fc017fb61173bd307cea95   \n",
       "...                                         ...   \n",
       "39995  1b977c00fa841ed8c542b9a1b4ad088293f6c7c6   \n",
       "39996  b935a4a925089bf0ceb2036fdd87c17ddef89651   \n",
       "39997  2619ebf9c87ce668bd78e9b816d5ac3e5831af66   \n",
       "39998  14f5937595f891ff505aa906a418f3d61b8277c2   \n",
       "39999  45bbba42c344150aa18c4735277e8fa6b94e1018   \n",
       "\n",
       "                                                 taxi_id  \\\n",
       "0      9f2cb5811cb0450b60400085b275296a3a307c2c26458f...   \n",
       "1      501280246f63bb05a144c72919f7c442c3a50fd9cfc2c7...   \n",
       "2      c26296b24f8bf5e5dd0751206051b0bac76a39d87ec36d...   \n",
       "3      1833edab9ed696386a8224986104fd046eaf9ea8dffb9e...   \n",
       "4      bf2d7795769ed240df920dce5605bda9e543e9cf05272a...   \n",
       "...                                                  ...   \n",
       "39995  34a67a4a44a0c07c24568bbdee112b8e7877b6b49664a7...   \n",
       "39996  c0233d1acdf95f778faf4bcadcff7035dc6bbcab99f07e...   \n",
       "39997  e931cc8f41df1e511815b487341e4896fb01f01b2f8fc6...   \n",
       "39998  dd3ec76231a679ee3501f302592955447df6907cf2806e...   \n",
       "39999  e11ecc8e74a2d3cbd2e9f58151ed09c81587cb491c50f6...   \n",
       "\n",
       "           trip_start_timestamp        trip_end_timestamp  trip_seconds  \\\n",
       "0     2019-05-09 13:00:00+00:00 2019-05-09 13:15:00+00:00           426   \n",
       "1     2020-02-20 18:15:00+00:00 2020-02-20 18:30:00+00:00           585   \n",
       "2     2019-05-31 11:45:00+00:00 2019-05-31 11:45:00+00:00           315   \n",
       "3     2019-01-10 02:15:00+00:00 2019-01-10 02:15:00+00:00           420   \n",
       "4     2021-12-17 16:15:00+00:00 2021-12-17 19:00:00+00:00          9720   \n",
       "...                         ...                       ...           ...   \n",
       "39995 2021-10-14 17:30:00+00:00 2021-10-14 18:30:00+00:00          3918   \n",
       "39996 2023-07-13 12:45:00+00:00 2023-07-13 12:45:00+00:00           480   \n",
       "39997 2019-07-26 10:45:00+00:00 2019-07-26 10:45:00+00:00           480   \n",
       "39998 2019-07-18 13:00:00+00:00 2019-07-18 13:15:00+00:00           660   \n",
       "39999 2021-02-07 18:45:00+00:00 2021-02-07 18:45:00+00:00           480   \n",
       "\n",
       "       trip_miles  pickup_census_tract  dropoff_census_tract  \\\n",
       "0            0.74          17031081402           17031081401   \n",
       "1            1.57          17031081700           17031080201   \n",
       "2            0.61          17031839100           17031839100   \n",
       "3            0.10                 <NA>                  <NA>   \n",
       "4          104.30                 <NA>                  <NA>   \n",
       "...           ...                  ...                   ...   \n",
       "39995       18.04          17031081500           17031980000   \n",
       "39996        1.50          17031081201           17031081800   \n",
       "39997        1.20          17031839100           17031081403   \n",
       "39998        0.90          17031320100           17031081800   \n",
       "39999        1.70                 <NA>                  <NA>   \n",
       "\n",
       "       pickup_community_area  dropoff_community_area  ...  extras  trip_total  \\\n",
       "0                          8                       8  ...     0.0        6.00   \n",
       "1                          8                       8  ...     0.0       11.25   \n",
       "2                         32                      32  ...     0.0        5.50   \n",
       "3                          8                      24  ...     0.0       10.25   \n",
       "4                       <NA>                    <NA>  ...    25.0      269.75   \n",
       "...                      ...                     ...  ...     ...         ...   \n",
       "39995                      8                      76  ...     1.0       48.75   \n",
       "39996                      8                       8  ...     0.0        7.50   \n",
       "39997                     32                       8  ...     0.0        9.00   \n",
       "39998                     32                       8  ...     0.0        7.25   \n",
       "39999                      1                      77  ...     0.0        8.50   \n",
       "\n",
       "       payment_type                        company  pickup_latitude  \\\n",
       "0              Cash                      Flash Cab        41.891972   \n",
       "1       Credit Card                      Flash Cab        41.892042   \n",
       "2              Cash      Chicago Carriage Cab Corp        41.880994   \n",
       "3       Credit Card      Taxi Affiliation Services        41.899602   \n",
       "4              Cash  Taxicab Insurance Agency, LLC              NaN   \n",
       "...             ...                            ...              ...   \n",
       "39995          Cash                      Flash Cab        41.892508   \n",
       "39996          Cash                      U Taxicab        41.899156   \n",
       "39997   Credit Card        Choice Taxi Association        41.880994   \n",
       "39998          Cash            Top Cab Affiliation        41.884987   \n",
       "39999       Unknown      Taxi Affiliation Services        42.009623   \n",
       "\n",
       "      pickup_longitude                       pickup_location  \\\n",
       "0           -87.612945  POINT (-87.6129454143 41.8919715078)   \n",
       "1           -87.631864  POINT (-87.6318639497 41.8920421365)   \n",
       "2           -87.632746  POINT (-87.6327464887 41.8809944707)   \n",
       "3           -87.633308   POINT (-87.6333080367 41.899602111)   \n",
       "4                  NaN                                  None   \n",
       "...                ...                                   ...   \n",
       "39995       -87.626215  POINT (-87.6262149064 41.8925077809)   \n",
       "39996       -87.626211  POINT (-87.6262105324 41.8991556134)   \n",
       "39997       -87.632746  POINT (-87.6327464887 41.8809944707)   \n",
       "39998       -87.620993  POINT (-87.6209929134 41.8849871918)   \n",
       "39999       -87.670167  POINT (-87.6701668569 42.0096228806)   \n",
       "\n",
       "       dropoff_latitude  dropoff_longitude  \\\n",
       "0             41.895033         -87.619711   \n",
       "1             41.909496         -87.630964   \n",
       "2             41.880994         -87.632746   \n",
       "3             41.901207         -87.676356   \n",
       "4                   NaN                NaN   \n",
       "...                 ...                ...   \n",
       "39995         41.979071         -87.903040   \n",
       "39996         41.893216         -87.637844   \n",
       "39997         41.890922         -87.618868   \n",
       "39998         41.893216         -87.637844   \n",
       "39999         41.986712         -87.663416   \n",
       "\n",
       "                               dropoff_location  \n",
       "0          POINT (-87.6197106717 41.8950334495)  \n",
       "1           POINT (-87.630963601 41.9094956686)  \n",
       "2          POINT (-87.6327464887 41.8809944707)  \n",
       "3      POINT (-87.6763559892 41.90120699410001)  \n",
       "4                                          None  \n",
       "...                                         ...  \n",
       "39995      POINT (-87.9030396611 41.9790708201)  \n",
       "39996      POINT (-87.6378442095 41.8932163595)  \n",
       "39997      POINT (-87.6188683546 41.8909220259)  \n",
       "39998      POINT (-87.6378442095 41.8932163595)  \n",
       "39999      POINT (-87.6634164054 41.9867117999)  \n",
       "\n",
       "[40000 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "client = bigquery.Client()\n",
    "# Define the SQL query\n",
    "query = \"\"\"\n",
    "SELECT * FROM bigquery-public-data.chicago_taxi_trips.taxi_trips\n",
    "WHERE EXTRACT(year FROM trip_start_timestamp) > 2018 AND\n",
    "      trip_miles IS NOT NULL AND\n",
    "      trip_seconds IS NOT NULL\n",
    "ORDER BY RAND()\n",
    "LIMIT 40000\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "query_job = client.query(query)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_key                    0\n",
       "taxi_id                       0\n",
       "trip_start_timestamp          0\n",
       "trip_end_timestamp            0\n",
       "trip_seconds                  0\n",
       "trip_miles                    0\n",
       "pickup_census_tract       19660\n",
       "dropoff_census_tract      19910\n",
       "pickup_community_area      2727\n",
       "dropoff_community_area     4131\n",
       "fare                         22\n",
       "tips                         22\n",
       "tolls                       294\n",
       "extras                       22\n",
       "trip_total                   22\n",
       "payment_type                  0\n",
       "company                       0\n",
       "pickup_latitude            2721\n",
       "pickup_longitude           2721\n",
       "pickup_location            2721\n",
       "dropoff_latitude           3964\n",
       "dropoff_longitude          3964\n",
       "dropoff_location           3964\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"trip_start_day\"] = df.trip_start_timestamp.apply(lambda x: x.day)\n",
    "df[\"trip_end_day\"] = df.trip_end_timestamp.apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"trip_start_month\"] = df.trip_start_timestamp.apply(lambda x: x.month)\n",
    "df[\"trip_end_month\"] = df.trip_end_timestamp.apply(lambda x: x.month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"trip_start_hour\"] = df.trip_start_timestamp.apply(lambda x: x.hour)\n",
    "df[\"trip_end_hour\"] = df.trip_end_timestamp.apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_location</th>\n",
       "      <th>trip_start_day</th>\n",
       "      <th>trip_start_month</th>\n",
       "      <th>trip_start_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4b0ff5638ed5905d99da624abc903b967f868cfd</td>\n",
       "      <td>3426c4a607f9804cc5f33ecb1cab61f70f63b3f0593c94...</td>\n",
       "      <td>2019-03-11 12:45:00+00:00</td>\n",
       "      <td>2019-03-11 12:45:00+00:00</td>\n",
       "      <td>340</td>\n",
       "      <td>0.84</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031280100</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>24 Seven Taxi</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>41.885300</td>\n",
       "      <td>-87.642808</td>\n",
       "      <td>POINT (-87.6428084655 41.8853000224)</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb868659d19997db5a97e8ae22097ce12fa3c8e0</td>\n",
       "      <td>5776c9e3fe3235f1c036220f324b07aa8728ebd6641bfc...</td>\n",
       "      <td>2019-03-11 22:45:00+00:00</td>\n",
       "      <td>2019-03-11 23:15:00+00:00</td>\n",
       "      <td>1213</td>\n",
       "      <td>7.66</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "      <td>41.878866</td>\n",
       "      <td>-87.625192</td>\n",
       "      <td>POINT (-87.6251921424 41.8788655841)</td>\n",
       "      <td>41.944227</td>\n",
       "      <td>-87.655998</td>\n",
       "      <td>POINT (-87.6559981815 41.9442266014)</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3cd47551b2424db69e6923b7ae4929fd7f04eed2</td>\n",
       "      <td>40dd26181941ac03ca95b3d4cf2f0d12b34ae0338e688b...</td>\n",
       "      <td>2023-05-12 07:00:00+00:00</td>\n",
       "      <td>2023-05-12 07:15:00+00:00</td>\n",
       "      <td>1134</td>\n",
       "      <td>5.47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>Flash Cab</td>\n",
       "      <td>41.900070</td>\n",
       "      <td>-87.720918</td>\n",
       "      <td>POINT (-87.7209182385 41.9000696026)</td>\n",
       "      <td>41.874005</td>\n",
       "      <td>-87.663518</td>\n",
       "      <td>POINT (-87.6635175498 41.874005383)</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5ded2218e2090f75607f2b03d00265ef69d9d71f</td>\n",
       "      <td>1d72db3a18692cc5b5e4ea41bb1de0e45d4149b495dbd0...</td>\n",
       "      <td>2021-06-13 12:45:00+00:00</td>\n",
       "      <td>2021-06-13 13:00:00+00:00</td>\n",
       "      <td>563</td>\n",
       "      <td>1.62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>Flash Cab</td>\n",
       "      <td>41.965812</td>\n",
       "      <td>-87.655879</td>\n",
       "      <td>POINT (-87.6558787862 41.96581197)</td>\n",
       "      <td>41.986712</td>\n",
       "      <td>-87.663416</td>\n",
       "      <td>POINT (-87.6634164054 41.9867117999)</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67fd7d0b6a607a3d228738481960eb1dd14079ad</td>\n",
       "      <td>48c3c22d766613be3982924a72efdc03758b9666b3996b...</td>\n",
       "      <td>2023-01-09 10:30:00+00:00</td>\n",
       "      <td>2023-01-09 10:45:00+00:00</td>\n",
       "      <td>420</td>\n",
       "      <td>1.40</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>Taxicab Insurance Agency, LLC</td>\n",
       "      <td>41.874005</td>\n",
       "      <td>-87.663518</td>\n",
       "      <td>POINT (-87.6635175498 41.874005383)</td>\n",
       "      <td>41.874005</td>\n",
       "      <td>-87.663518</td>\n",
       "      <td>POINT (-87.6635175498 41.874005383)</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>fe24c1fa80ac672a7c691ec4e372349dedb12c82</td>\n",
       "      <td>1aa27355c709dc0cba3790a2549861c6b7864165ccf020...</td>\n",
       "      <td>2022-11-02 18:00:00+00:00</td>\n",
       "      <td>2022-11-02 18:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.689730</td>\n",
       "      <td>-87.669054</td>\n",
       "      <td>POINT (-87.6690544032 41.6897299145)</td>\n",
       "      <td>41.689730</td>\n",
       "      <td>-87.669054</td>\n",
       "      <td>POINT (-87.6690544032 41.6897299145)</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>149b3c85da0c94caa99aab8a3d6baf5a0299aec3</td>\n",
       "      <td>91996675edbfbd4ed70014061e791e1f42a90050307183...</td>\n",
       "      <td>2019-08-16 16:00:00+00:00</td>\n",
       "      <td>2019-08-16 16:30:00+00:00</td>\n",
       "      <td>1560</td>\n",
       "      <td>0.10</td>\n",
       "      <td>17031071500</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.914616</td>\n",
       "      <td>-87.631717</td>\n",
       "      <td>POINT (-87.6317173661 41.9146162864)</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>f680d345c22ff8a161db7f8349f7b609d650b634</td>\n",
       "      <td>878fad758bcb6e1a1444c81a48594c00962e442e20bb71...</td>\n",
       "      <td>2020-02-18 18:15:00+00:00</td>\n",
       "      <td>2020-02-18 18:30:00+00:00</td>\n",
       "      <td>861</td>\n",
       "      <td>1.58</td>\n",
       "      <td>17031081800</td>\n",
       "      <td>17031081300</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "      <td>41.893216</td>\n",
       "      <td>-87.637844</td>\n",
       "      <td>POINT (-87.6378442095 41.8932163595)</td>\n",
       "      <td>41.898332</td>\n",
       "      <td>-87.620763</td>\n",
       "      <td>POINT (-87.6207628651 41.8983317935)</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>c180c8dd0502925daf32f79221cc68c92a287b77</td>\n",
       "      <td>d899402f96aeec3adabbee95f3a99aba9ade0e0b5c1d93...</td>\n",
       "      <td>2019-03-05 09:15:00+00:00</td>\n",
       "      <td>2019-03-05 09:30:00+00:00</td>\n",
       "      <td>1087</td>\n",
       "      <td>2.90</td>\n",
       "      <td>17031080100</td>\n",
       "      <td>17031281900</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>City Service</td>\n",
       "      <td>41.907520</td>\n",
       "      <td>-87.626659</td>\n",
       "      <td>POINT (-87.6266589003 41.90752007470001)</td>\n",
       "      <td>41.879255</td>\n",
       "      <td>-87.642649</td>\n",
       "      <td>POINT (-87.642648998 41.8792550844)</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>2a66780f8552c9f9d4d8211bc68b755808808c61</td>\n",
       "      <td>4c437b70b55212de32439053c915572d336a8cc2fb7129...</td>\n",
       "      <td>2022-04-21 16:00:00+00:00</td>\n",
       "      <td>2022-04-21 16:00:00+00:00</td>\n",
       "      <td>420</td>\n",
       "      <td>1.00</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031081403</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>41.890922</td>\n",
       "      <td>-87.618868</td>\n",
       "      <td>POINT (-87.6188683546 41.8909220259)</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     unique_key  \\\n",
       "0      4b0ff5638ed5905d99da624abc903b967f868cfd   \n",
       "1      eb868659d19997db5a97e8ae22097ce12fa3c8e0   \n",
       "2      3cd47551b2424db69e6923b7ae4929fd7f04eed2   \n",
       "3      5ded2218e2090f75607f2b03d00265ef69d9d71f   \n",
       "4      67fd7d0b6a607a3d228738481960eb1dd14079ad   \n",
       "...                                         ...   \n",
       "39995  fe24c1fa80ac672a7c691ec4e372349dedb12c82   \n",
       "39996  149b3c85da0c94caa99aab8a3d6baf5a0299aec3   \n",
       "39997  f680d345c22ff8a161db7f8349f7b609d650b634   \n",
       "39998  c180c8dd0502925daf32f79221cc68c92a287b77   \n",
       "39999  2a66780f8552c9f9d4d8211bc68b755808808c61   \n",
       "\n",
       "                                                 taxi_id  \\\n",
       "0      3426c4a607f9804cc5f33ecb1cab61f70f63b3f0593c94...   \n",
       "1      5776c9e3fe3235f1c036220f324b07aa8728ebd6641bfc...   \n",
       "2      40dd26181941ac03ca95b3d4cf2f0d12b34ae0338e688b...   \n",
       "3      1d72db3a18692cc5b5e4ea41bb1de0e45d4149b495dbd0...   \n",
       "4      48c3c22d766613be3982924a72efdc03758b9666b3996b...   \n",
       "...                                                  ...   \n",
       "39995  1aa27355c709dc0cba3790a2549861c6b7864165ccf020...   \n",
       "39996  91996675edbfbd4ed70014061e791e1f42a90050307183...   \n",
       "39997  878fad758bcb6e1a1444c81a48594c00962e442e20bb71...   \n",
       "39998  d899402f96aeec3adabbee95f3a99aba9ade0e0b5c1d93...   \n",
       "39999  4c437b70b55212de32439053c915572d336a8cc2fb7129...   \n",
       "\n",
       "           trip_start_timestamp        trip_end_timestamp  trip_seconds  \\\n",
       "0     2019-03-11 12:45:00+00:00 2019-03-11 12:45:00+00:00           340   \n",
       "1     2019-03-11 22:45:00+00:00 2019-03-11 23:15:00+00:00          1213   \n",
       "2     2023-05-12 07:00:00+00:00 2023-05-12 07:15:00+00:00          1134   \n",
       "3     2021-06-13 12:45:00+00:00 2021-06-13 13:00:00+00:00           563   \n",
       "4     2023-01-09 10:30:00+00:00 2023-01-09 10:45:00+00:00           420   \n",
       "...                         ...                       ...           ...   \n",
       "39995 2022-11-02 18:00:00+00:00 2022-11-02 18:00:00+00:00             0   \n",
       "39996 2019-08-16 16:00:00+00:00 2019-08-16 16:30:00+00:00          1560   \n",
       "39997 2020-02-18 18:15:00+00:00 2020-02-18 18:30:00+00:00           861   \n",
       "39998 2019-03-05 09:15:00+00:00 2019-03-05 09:30:00+00:00          1087   \n",
       "39999 2022-04-21 16:00:00+00:00 2022-04-21 16:00:00+00:00           420   \n",
       "\n",
       "       trip_miles  pickup_census_tract  dropoff_census_tract  \\\n",
       "0            0.84          17031839100           17031280100   \n",
       "1            7.66                 <NA>                  <NA>   \n",
       "2            5.47                 <NA>                  <NA>   \n",
       "3            1.62                 <NA>                  <NA>   \n",
       "4            1.40                 <NA>                  <NA>   \n",
       "...           ...                  ...                   ...   \n",
       "39995        0.00                 <NA>                  <NA>   \n",
       "39996        0.10          17031071500           17031839100   \n",
       "39997        1.58          17031081800           17031081300   \n",
       "39998        2.90          17031080100           17031281900   \n",
       "39999        1.00          17031839100           17031081403   \n",
       "\n",
       "       pickup_community_area  dropoff_community_area  ...  \\\n",
       "0                         32                      28  ...   \n",
       "1                         32                       6  ...   \n",
       "2                         23                      28  ...   \n",
       "3                          3                      77  ...   \n",
       "4                         28                      28  ...   \n",
       "...                      ...                     ...  ...   \n",
       "39995                     75                      75  ...   \n",
       "39996                      7                      32  ...   \n",
       "39997                      8                       8  ...   \n",
       "39998                      8                      28  ...   \n",
       "39999                     32                       8  ...   \n",
       "\n",
       "                             company  pickup_latitude  pickup_longitude  \\\n",
       "0                      24 Seven Taxi        41.880994        -87.632746   \n",
       "1          Chicago Carriage Cab Corp        41.878866        -87.625192   \n",
       "2                          Flash Cab        41.900070        -87.720918   \n",
       "3                          Flash Cab        41.965812        -87.655879   \n",
       "4      Taxicab Insurance Agency, LLC        41.874005        -87.663518   \n",
       "...                              ...              ...               ...   \n",
       "39995      Taxi Affiliation Services        41.689730        -87.669054   \n",
       "39996      Taxi Affiliation Services        41.914616        -87.631717   \n",
       "39997      Chicago Carriage Cab Corp        41.893216        -87.637844   \n",
       "39998                   City Service        41.907520        -87.626659   \n",
       "39999      Taxi Affiliation Services        41.880994        -87.632746   \n",
       "\n",
       "                                pickup_location  dropoff_latitude  \\\n",
       "0          POINT (-87.6327464887 41.8809944707)         41.885300   \n",
       "1          POINT (-87.6251921424 41.8788655841)         41.944227   \n",
       "2          POINT (-87.7209182385 41.9000696026)         41.874005   \n",
       "3            POINT (-87.6558787862 41.96581197)         41.986712   \n",
       "4           POINT (-87.6635175498 41.874005383)         41.874005   \n",
       "...                                         ...               ...   \n",
       "39995      POINT (-87.6690544032 41.6897299145)         41.689730   \n",
       "39996      POINT (-87.6317173661 41.9146162864)         41.880994   \n",
       "39997      POINT (-87.6378442095 41.8932163595)         41.898332   \n",
       "39998  POINT (-87.6266589003 41.90752007470001)         41.879255   \n",
       "39999      POINT (-87.6327464887 41.8809944707)         41.890922   \n",
       "\n",
       "      dropoff_longitude                      dropoff_location  trip_start_day  \\\n",
       "0            -87.642808  POINT (-87.6428084655 41.8853000224)              11   \n",
       "1            -87.655998  POINT (-87.6559981815 41.9442266014)              11   \n",
       "2            -87.663518   POINT (-87.6635175498 41.874005383)              12   \n",
       "3            -87.663416  POINT (-87.6634164054 41.9867117999)              13   \n",
       "4            -87.663518   POINT (-87.6635175498 41.874005383)               9   \n",
       "...                 ...                                   ...             ...   \n",
       "39995        -87.669054  POINT (-87.6690544032 41.6897299145)               2   \n",
       "39996        -87.632746  POINT (-87.6327464887 41.8809944707)              16   \n",
       "39997        -87.620763  POINT (-87.6207628651 41.8983317935)              18   \n",
       "39998        -87.642649   POINT (-87.642648998 41.8792550844)               5   \n",
       "39999        -87.618868  POINT (-87.6188683546 41.8909220259)              21   \n",
       "\n",
       "       trip_start_month trip_start_hour  \n",
       "0                     3              12  \n",
       "1                     3              22  \n",
       "2                     5               7  \n",
       "3                     6              12  \n",
       "4                     1              10  \n",
       "...                 ...             ...  \n",
       "39995                11              18  \n",
       "39996                 8              16  \n",
       "39997                 2              18  \n",
       "39998                 3               9  \n",
       "39999                 4              16  \n",
       "\n",
       "[40000 rows x 26 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"Final_Chicago_Train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mlops_final_run/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mlops_final_run' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "Copying file://Final_Chicago_Train.csv [Content-Type=text/csv]...\n",
      "- [1 files][ 16.4 MiB/ 16.4 MiB]                                                \n",
      "Operation completed over 1 objects/16.4 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l {GOOGLE_CLOUD_REGION} gs://{GCS_BUCKET_NAME}\n",
    "!gsutil cp Final_Chicago_Train.csv {DATA_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyBh_2LO4Ccw"
   },
   "source": [
    "# Write Example Component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOjDv93eS5xV"
   },
   "source": [
    "### Write model code.\n",
    "\n",
    "We will use the same model code as in the\n",
    "[Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aES7Hv5QTDK3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_trainer_module_file = 'bfs_trainer.py'\n",
    "_transformer_module_file = 'transformer.py'\n",
    "_training_pipeline_file = 'training_pipeline.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OB3fD-DSuAJc",
    "outputId": "2ef1dc42-31f1-4081-dd9a-0910c8208ccb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_transformer_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Define the numerical features that will be used in the model.\n",
    "NUMERICAL_FEATURES = ['trip_miles', 'trip_seconds']\n",
    "\n",
    "# Define the features that will be bucketized.\n",
    "BUCKET_FEATURES = [\n",
    "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "    'dropoff_longitude'\n",
    "]\n",
    "\n",
    "# Define the number of buckets used by tf.transform for encoding each feature in BUCKET_FEATURES.\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "# Define the categorical features that are represented as numerical values.\n",
    "CATEGORICAL_NUMERICAL_FEATURES = [\n",
    "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
    "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
    "    'dropoff_community_area'\n",
    "]\n",
    "\n",
    "# Define the categorical features that are represented as strings.\n",
    "CATEGORICAL_STRING_FEATURES = [\n",
    "    'payment_type',\n",
    "    'company',\n",
    "]\n",
    "\n",
    "# Define the number of vocabulary terms used for encoding categorical features.\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Define the count of out-of-vocab buckets in which unrecognized categorical are hashed.\n",
    "OOV_SIZE = 10\n",
    "\n",
    "# Define the keys for the label and fare columns in the input data.\n",
    "LABEL_KEY = 'fare'\n",
    "\n",
    "# Define a helper function that appends the suffix '_xf' to a feature key to avoid clashes\n",
    "# with raw feature keys when running the Evaluator component.\n",
    "def t_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "def _make_one_hot(x, key):\n",
    "    \"\"\"Make a one-hot tensor to encode categorical features.\n",
    "    Args:\n",
    "        x: A dense tensor\n",
    "        key: A string key for the feature in the input\n",
    "    Returns:\n",
    "        A dense one-hot tensor as a float list\n",
    "    \"\"\"\n",
    "    # Computing and applying vocabulary to the input tensor and integerizing it.\n",
    "    integerized = tft.compute_and_apply_vocabulary(x,\n",
    "                                                   top_k=VOCAB_SIZE,\n",
    "                                                   num_oov_buckets=OOV_SIZE,\n",
    "                                                   vocab_filename=key,\n",
    "                                                   name=key)\n",
    "    # Getting the vocabulary size for the feature.\n",
    "    depth = (\n",
    "        tft.experimental.get_vocabulary_size_by_name(key) + OOV_SIZE)\n",
    "    # Converting the integerized tensor to a one-hot tensor.\n",
    "    one_hot_encoded = tf.one_hot(\n",
    "        integerized,\n",
    "        depth=tf.cast(depth, tf.int32),\n",
    "        on_value=1.0,\n",
    "        off_value=0.0)\n",
    "    return tf.reshape(one_hot_encoded, [-1, depth])\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "    \"\"\"Replace missing values in a SparseTensor.\n",
    "    Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "    Args:\n",
    "      x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "        in the second dimension.\n",
    "    Returns:\n",
    "      A rank 1 tensor where missing values of `x` have been filled in.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, tf.sparse.SparseTensor):\n",
    "        return x\n",
    "\n",
    "    default_value = '' if x.dtype == tf.string else 0\n",
    "    return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "      inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "      Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    for key in NUMERICAL_FEATURES:\n",
    "        # Filling in missing values and scaling the numerical features to have mean=0 and variance=1.\n",
    "        outputs[t_name(key)] = tft.scale_to_z_score(\n",
    "            _fill_in_missing(inputs[key]), name=key)\n",
    "\n",
    "    for key in BUCKET_FEATURES:\n",
    "        # Filling in missing values and bucketizing the features.\n",
    "        outputs[t_name(key)] = tf.cast(tft.bucketize(\n",
    "            _fill_in_missing(inputs[key]), FEATURE_BUCKET_COUNT, name=key),\n",
    "            dtype=tf.float32)\n",
    "\n",
    "    for key in CATEGORICAL_STRING_FEATURES:\n",
    "        # Filling in missing values and one-hot encoding the categorical string features.\n",
    "        outputs[t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n",
    "\n",
    "    for key in CATEGORICAL_NUMERICAL_FEATURES:\n",
    "        # Filling in missing values, converting the categorical numerical features to strings, and one-hot encoding them.\n",
    "        outputs[t_name(key)] = _make_one_hot(tf.strings.strip(\n",
    "        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n",
    "\n",
    "    # Fare is used as a label here\n",
    "    outputs[LABEL_KEY] = _fill_in_missing(inputs[LABEL_KEY])\n",
    "\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gnc67uQNTDfW",
    "outputId": "46575526-ccab-4867-b7c5-85b85e49d8b0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bfs_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "# Import necessary libraries\n",
    "from typing import Dict, List, Text\n",
    "import os\n",
    "import glob\n",
    "from absl import logging\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_transform import TFTransformOutput\n",
    "\n",
    "\n",
    "_LABEL_KEY = 'fare'\n",
    "\n",
    "_BATCH_SIZE = 40\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "      file_pattern: List of paths or patterns of input tfrecord files.\n",
    "      data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "      tf_transform_output: A TFTransformOutput.\n",
    "      batch_size: representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "      A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    # Create a dataset from the input files using the TFTransformOutput and batch size\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(\n",
    "            batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "        tf_transform_output.transformed_metadata.schema)\n",
    "\n",
    "def _get_tf_examples_serving_signature(model, tf_transform_output):\n",
    "    \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n",
    "\n",
    "    # We need to track the layers in the model in order to save it.\n",
    "    # TODO(b/162357359): Revise once the bug is resolved.\n",
    "    model.tft_layer_inference = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_example):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        # Remove label feature since these will not be present at serving time.\n",
    "        raw_feature_spec.pop(_LABEL_KEY)\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_inference(raw_features)\n",
    "        logging.info('serve_transformed_features = %s', transformed_features)\n",
    "\n",
    "        outputs = model(transformed_features)\n",
    "        # TODO(b/154085620): Convert the predicted labels from the model using a\n",
    "        # reverse-lookup (opposite of transform.py).\n",
    "        return {'outputs': outputs}\n",
    "\n",
    "    # Define a serving function that takes in serialized tf.Example and returns model outputs\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def _get_transform_features_signature(model, tf_transform_output):\n",
    "    \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n",
    "\n",
    "    # We need to track the layers in the model in order to save it.\n",
    "    # TODO(b/162357359): Revise once the bug is resolved.\n",
    "    model.tft_layer_eval = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def transform_features_fn(serialized_tf_example):\n",
    "        \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_eval(raw_features)\n",
    "        logging.info('eval_transformed_features = %s', transformed_features)\n",
    "        return transformed_features\n",
    "\n",
    "    # Define a serving function that takes in serialized tf.Example and returns transformed features\n",
    "    return transform_features_fn\n",
    "\n",
    "def export_serving_model(tf_transform_output, model, output_dir):\n",
    "    \"\"\"Exports a keras model for serving.\n",
    "\n",
    "    Args:\n",
    "      tf_transform_output: Wrapper around output of tf.Transform.\n",
    "      model: A keras model to export for serving.\n",
    "      output_dir: A directory where the model will be exported to.\n",
    "    \"\"\"\n",
    "    # Save the transform layer to the model for serving\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "            _get_tf_examples_serving_signature(model, tf_transform_output),\n",
    "        'transform_features':\n",
    "            _get_transform_features_signature(model, tf_transform_output),\n",
    "    }\n",
    "\n",
    "    # Save the model with serving signatures\n",
    "    model.save(output_dir, save_format='tf', signatures=signatures)\n",
    "\n",
    "def _build_keras_model(tf_transform_output: TFTransformOutput\n",
    "                       ) -> tf.keras.Model:\n",
    "    \"\"\"Creates a DNN Keras model for classifying taxi data.\n",
    "\n",
    "    Args:\n",
    "      tf_transform_output: [TFTransformOutput], the outputs from Transform\n",
    "\n",
    "    Returns:\n",
    "      A keras Model.\n",
    "    \"\"\"\n",
    "    # Create a dictionary of model inputs based on the transformed feature spec\n",
    "    feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "    feature_spec.pop(_LABEL_KEY)\n",
    "\n",
    "    inputs = {}\n",
    "    for key, spec in feature_spec.items():\n",
    "        if isinstance(spec, tf.io.VarLenFeature):\n",
    "            inputs[key] = tf.keras.layers.Input(shape=[None], name=key, dtype=spec.dtype, sparse=True)\n",
    "        elif isinstance(spec, tf.io.FixedLenFeature):\n",
    "            inputs[key] = tf.keras.layers.Input(shape=spec.shape or [1], name=key, dtype=spec.dtype)\n",
    "        else:\n",
    "            raise ValueError('Spec type is not supported: ', key, spec)\n",
    "\n",
    "    # Define the model architecture using the inputs\n",
    "    output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n",
    "    output = tf.keras.layers.Dense(100, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dense(70, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dense(50, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dense(20, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dense(1)(output)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "      fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    # Create training and evaluation datasets using the input function\n",
    "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n",
    "                              tf_transform_output, _BATCH_SIZE)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n",
    "                             tf_transform_output, _BATCH_SIZE)\n",
    "\n",
    "    # Build and compile the Keras model\n",
    "    model = _build_keras_model(tf_transform_output)\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=tf.optimizers.Adam(learning_rate=0.0005), \n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    # Train the model using the training and evaluation datasets\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=fn_args.model_run_dir, update_freq='batch')\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[tensorboard_callback])\n",
    "\n",
    "    # Export the trained model with serving signatures\n",
    "    export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j54Ya94tR1-"
   },
   "source": [
    "### Copy files to bucket\n",
    "The transform and trainer module files need to be copied over to the GCP bucket for TFX to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMMs5wuNYAbc",
    "outputId": "d120a9c3-c107-47d2-bf8a-961f62f1502f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://bfs_trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  7.3 KiB/  7.3 KiB]                                                \n",
      "Operation completed over 1 objects/7.3 KiB.                                      \n",
      "Copying file://transformer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  4.4 KiB/  4.4 KiB]                                                \n",
      "Operation completed over 1 objects/4.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp bfs_trainer.py {MODULE_ROOT}/\n",
    "!gsutil cp transformer.py {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-3jxJZcI7q-"
   },
   "source": [
    "### Create TFX pipeline. This pipeline can then be passed onto an orchestrator, such as KubeFlow, for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline\n",
      "Creating example_gen\n",
      "Creating statistics_gen\n",
      "Creating schema_gen\n",
      "Creating example_validator\n",
      "Creating transform\n",
      "Creating trainer\n",
      "Adding model_resolver\n",
      "Adding evaluator\n",
      "Creating pusher\n",
      "Creating tfx_pipeline\n",
      "Creating runner\n",
      "Executing runner\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying transformer.py -> build/lib\n",
      "installing to /var/tmp/tmpbea9d4mu\n",
      "running install\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install_lib\n",
      "copying build/lib/transformer.py -> /var/tmp/tmpbea9d4mu\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /var/tmp/tmpbea9d4mu/tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmpbea9d4mu/tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/WHEEL\n",
      "creating '/var/tmp/tmpc_xjpfbm/tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de-py3-none-any.whl' and adding '/var/tmp/tmpbea9d4mu' to it\n",
      "adding 'transformer.py'\n",
      "adding 'tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/RECORD'\n",
      "removing /var/tmp/tmpbea9d4mu\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying bfs_trainer.py -> build/lib\n",
      "installing to /var/tmp/tmp4i1spfdo\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/bfs_trainer.py -> /var/tmp/tmp4i1spfdo\n",
      "running install_egg_info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /var/tmp/tmp4i1spfdo/tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmp4i1spfdo/tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/WHEEL\n",
      "creating '/var/tmp/tmplkceb7o5/tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a-py3-none-any.whl' and adding '/var/tmp/tmp4i1spfdo' to it\n",
      "adding 'bfs_trainer.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/RECORD'\n",
      "removing /var/tmp/tmp4i1spfdo\n",
      "Pipeline is ready to be executed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_pipeline(pipeline_name, pipeline_root, serving_model_dir, data_root, file_name):\n",
    "    print(\"Running pipeline\")\n",
    "\n",
    "    print(\"Creating example_gen\")\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "    print(\"Creating statistics_gen\")\n",
    "    statistics_gen = tfx.components.StatisticsGen(examples=example_gen.outputs[\"examples\"])\n",
    "\n",
    "    print(\"Creating schema_gen\")\n",
    "    schema_gen = tfx.components.SchemaGen(statistics=statistics_gen.outputs[\"statistics\"])\n",
    "\n",
    "    print(\"Creating example_validator\")\n",
    "    example_validator = tfx.components.ExampleValidator(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "        schema=schema_gen.outputs[\"schema\"]\n",
    "    )\n",
    "\n",
    "    print(\"Creating transform\")\n",
    "    transform = tfx.components.Transform(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        module_file=os.path.join(MODULE_ROOT, \"transformer.py\")\n",
    "    )\n",
    "\n",
    "    print(\"Creating trainer\")\n",
    "    trainer = tfx.components.Trainer(\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        module_file=os.path.join(MODULE_ROOT, \"bfs_trainer.py\"),\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=150),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=150)\n",
    "    )\n",
    "\n",
    "    print(\"Adding model_resolver\")\n",
    "    model_resolver = tfx.dsl.Resolver(\n",
    "        strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "        model=tfx.dsl.Channel(type=standard_artifacts.Model),\n",
    "        model_blessing=tfx.dsl.Channel(type=standard_artifacts.ModelBlessing)\n",
    "    ).with_id('latest_blessed_model_resolver')\n",
    "\n",
    "    print(\"Adding evaluator\")\n",
    "    evaluator = tfx.components.Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=tfma.EvalConfig(\n",
    "            model_specs=[\n",
    "                tfma.ModelSpec(\n",
    "                    signature_name='serving_default',\n",
    "                    label_key='fare',\n",
    "                    preprocessing_function_names=['transform_features'],\n",
    "                )\n",
    "            ],\n",
    "            metrics_specs=[\n",
    "                tfma.MetricsSpec(\n",
    "                    metrics=[\n",
    "                        tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                        tfma.MetricConfig(class_name='MeanSquaredError')\n",
    "                    ]\n",
    "                )\n",
    "            ],\n",
    "            slicing_specs=[\n",
    "                tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Creating pusher\")\n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=evaluator.outputs['blessing'],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(base_directory=serving_model_dir)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Creating tfx_pipeline\")\n",
    "    tfx_pipeline = Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[\n",
    "            example_gen,\n",
    "            statistics_gen,\n",
    "            schema_gen,\n",
    "            example_validator,\n",
    "            transform,\n",
    "            trainer,\n",
    "            model_resolver,\n",
    "            evaluator,\n",
    "            pusher\n",
    "        ],\n",
    "        enable_cache=True\n",
    "    )\n",
    "\n",
    "    print(\"Creating runner\")\n",
    "    runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "        config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "        output_filename=pipeline_name + '_pipeline.json'\n",
    "    )\n",
    "\n",
    "    print(\"Executing runner\")\n",
    "    _ = runner.run(tfx_pipeline)\n",
    "\n",
    "    print(\"Pipeline is ready to be executed.\")\n",
    "    \n",
    "    \n",
    "build_pipeline(PIPELINE_NAME, PIPELINE_ROOT, SERVING_MODEL_DIR, DATA_ROOT, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Black Friday Sales pipeline using TFX",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
